{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24790971-7ddb-4d90-bc46-bea2cc00cb0b",
   "metadata": {},
   "source": [
    "#Â Building benchmark models\n",
    "\n",
    "**Author: Matthew Massett**\n",
    "\n",
    "Date: 17-03-2025\n",
    "\n",
    "This notebook provides instructions for constructing BLOSUM and RANDOM benchmark models using a contrived short peptide sequence.\n",
    "\n",
    "Table of contents\n",
    "-----------------\n",
    "1. [Building a tokeniser](#section1)\n",
    "1. [Building a Blosum sequence model](#section2)\n",
    "2. [Building a random sequence model](#section3)\n",
    "3. [Generating variant proteins](#section4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1fa644-65f9-434d-8b7e-4293d0c3b083",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -q prodigyprotein/dist/prodigyprotein-1.0.0.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da9cdae-2d7d-423c-8de6-a870f765d4d8",
   "metadata": {},
   "source": [
    "Prodigy Protein has defined Blosum probability embedding layers that can used to obtain the probability of each substitution at all positions. These take as input a protein sequence (tokenised) and return (for every position) the probabilities for substitutions. It operates similar to embedding layers in a sequence model, but with every token being mapped to a a unique probability vector. \n",
    "\n",
    "The sampler for variant generation and the BlosumProbabilityEmbedding layer is imported below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be928ef6-2652-4372-be89-8f6aebb03073",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prodigyprotein import (\n",
    "    WeightedDirectedEvolutionSampler, \n",
    "    BlosumProbabilityEmbedding\n",
    ")\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d039bc-e293-4c12-9710-d0c082743d7a",
   "metadata": {},
   "source": [
    "<a id=\"section1\"></a>\n",
    "\n",
    "## 1. Building a tokeniser\n",
    "\n",
    "A quick tokeniser is defined below to convert amino acid (AA) residues into integers. AA letters are ordered alphabetically. This tokeniser will not handle sequences of different lengths as both example benchmark models defined here do not have padding tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "863e22c3-0cf2-4894-a1e3-c97c52def146",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class blosum_tokeniser():\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.vocab = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L',\n",
    "                      'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y']\n",
    "    \n",
    "    def __call__(self, input):\n",
    "\n",
    "        tokenised = []\n",
    "        sequences = {}\n",
    "\n",
    "        # Strip whitespace from input strings\n",
    "        input = [i.replace(' ', '') for i in input]\n",
    "\n",
    "        # Get max length sequence in input\n",
    "\n",
    "        for sequence in input:\n",
    "            \n",
    "            tokens = tf.constant([self.vocab.index(aa) for aa in sequence])[None,]  \n",
    "            tokenised.append(tokens)\n",
    "    \n",
    "        # Copy tokenised tensor to n_variants\n",
    "        tokenised = tf.concat(tokenised, axis=0)\n",
    "        \n",
    "        sequences['input_ids'] = tokenised\n",
    "        sequences['attention_mask'] = tf.ones_like(tokenised)\n",
    "        \n",
    "        return sequences\n",
    "\n",
    "    def batch_decode(self,\n",
    "                     tokenised_sequence,\n",
    "                     skip_special_tokens=True):\n",
    "        \n",
    "        decoded_sequence = []\n",
    "        \n",
    "        for tokenised_seq in tokenised_sequence:\n",
    "            seq = ' '.join([self.vocab[i] for i in tokenised_seq])\n",
    "            decoded_sequence.append(seq)\n",
    "\n",
    "        return decoded_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9e50a5-fb47-4eb0-91a3-9f38aad2ad2a",
   "metadata": {},
   "source": [
    "The tokeniser enables the tokenisation of amino acid sequences to integers. Later, it will allow the conversion of generated variants back to AA letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b857ebef-dce6-434c-b825-d70704ffa913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
      "array([[0, 0],\n",
      "       [0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
      "array([[1, 1],\n",
      "       [1, 1]], dtype=int32)>}\n",
      "['A A', 'A A']\n"
     ]
    }
   ],
   "source": [
    "tokeniser = blosum_tokeniser()\n",
    "\n",
    "encoded = tokeniser(['AA', 'AA'])\n",
    "decoded = tokeniser.batch_decode(encoded['input_ids'])\n",
    "\n",
    "print(encoded)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0951ea-4ff9-4ccf-8505-4c2122ad4884",
   "metadata": {},
   "source": [
    "\n",
    "<a id=\"section2\"></a>\n",
    "\n",
    "## 2. Building a Blosum sequence model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "431844c4-d593-43e1-b3fa-5e9ace5819bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blosum_model(blosum):\n",
    "    '''\n",
    "    Returns Blosum TF model. Detects amino acid in sequence and retrieves probability\n",
    "    of substitution to the 20 possible amino acids. \n",
    "    '''\n",
    "    input_chains = tf.keras.layers.Input(shape=(None,), name='input_ids', dtype='int32')\n",
    "    input_ = tf.keras.layers.Input(shape=(None,), name='attention_mask',dtype='int32')\n",
    "    logits = BlosumProbabilityEmbedding(cluster_pc=blosum)(input_chains)\n",
    "    logits = tf.cast(logits, tf.float64)\n",
    "\n",
    "    blosum_model = tf.keras.models.Model(inputs={'input_ids': input_chains, 'attention_mask': input_}, \n",
    "                                         outputs={'logits': logits})\n",
    "\n",
    "    return blosum_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e59fe65-221f-4ff8-aff3-2ff7a1c41610",
   "metadata": {},
   "source": [
    "A blosum sequence model is constructed below. The probabilities returned will be in alphabetical order. **The DirectedEvolution samplers automatically detect whether a model returned probabilities or logits so this can be used immediately in the variant generation workflow.**\n",
    "\n",
    "Recall, transformers tokenisers return two outputs (default) consisting of the tokenised sequence and an attention mask. To make the model compatible with DirectedEvolution we make the model accept these as well. *Note the attention mask does not change the returned probabilities.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "53287c93-5391-41ed-a607-0d1a9ebf017a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)      [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " blosum_probability_embeddi  (None, None, 20)             400       ['input_ids[0][0]']           \n",
      " ng_1 (BlosumProbabilityEmb                                                                       \n",
      " edding)                                                                                          \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer  [(None, None)]               0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.cast_1 (TFOpLambda)      (None, None, 20)             0         ['blosum_probability_embedding\n",
      "                                                                    _1[0][0]']                    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 400 (1.56 KB)\n",
      "Trainable params: 0 (0.00 Byte)\n",
      "Non-trainable params: 400 (1.56 KB)\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "blosum_p_model = blosum_model(blosum=45)\n",
    "print(blosum_p_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41aaaee4-d605-4b1f-919b-db99a56cd20d",
   "metadata": {},
   "source": [
    "Lets test it out. Note the model takes a tokenised sequence of length `None`. This is so that it can accept arbitrarily long protein sequences. \n",
    "\n",
    "The model returns probabilities of shape `[None, None, 20]` corresponding to the 20 amino acid probabilities for each sequence, position and residue.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1c9fb0ae-2269-4717-ab59-7b11d70feaa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logits': <tf.Tensor: shape=(1, 3, 20), dtype=float64, numpy=\n",
      "array([[[0.48088914, 0.00497193, 0.04080733, 0.01465601, 0.02156289,\n",
      "         0.08338834, 0.00684768, 0.01202223, 0.00868969, 0.02247673,\n",
      "         0.01091138, 0.0148531 , 0.01872314, 0.00687112, 0.03291445,\n",
      "         0.07793844, 0.03591704, 0.03769072, 0.01068009, 0.05718854],\n",
      "        [0.48088914, 0.00497193, 0.04080733, 0.01465601, 0.02156289,\n",
      "         0.08338834, 0.00684768, 0.01202223, 0.00868969, 0.02247673,\n",
      "         0.01091138, 0.0148531 , 0.01872314, 0.00687112, 0.03291445,\n",
      "         0.07793844, 0.03591704, 0.03769072, 0.01068009, 0.05718854],\n",
      "        [0.07373622, 0.55200917, 0.03539559, 0.00898901, 0.02645044,\n",
      "         0.03616481, 0.00593956, 0.00737362, 0.00532967, 0.01949593,\n",
      "         0.00946435, 0.01288332, 0.00812007, 0.00421428, 0.02854943,\n",
      "         0.04780217, 0.03115383, 0.03269229, 0.00463187, 0.04960437]]])>}\n"
     ]
    }
   ],
   "source": [
    "# Sequence of tokens as input. Corresponding to a contrived short peptide sequence alanine, alanine, arginine.\n",
    "short_peptide = tf.constant([[0, 0, 1]])\n",
    "attention_mask = tf.constant([[1, 1, 1]])\n",
    "# Get probabilities from blosum model\n",
    "peptide_substitution_p = blosum_p_model({\n",
    "    'input_ids': short_peptide,\n",
    "    'attention_mask': attention_mask\n",
    "} )\n",
    "\n",
    "print(peptide_substitution_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1911ced9-95e1-4ddd-b8bc-6186591c6ae3",
   "metadata": {},
   "source": [
    "The probabilties returned from the model are the same for the first two positions as expected since they are the same amino acid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31459bc3-3ba4-443a-bb00-fc592b8cbbca",
   "metadata": {},
   "source": [
    "<a id=\"section3\"></a>\n",
    "\n",
    "## Building a Random sequence model\n",
    "\n",
    "Lets build a model that returns the same logit for every position and residue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8ce3c43f-42db-44d3-afe9-784120aa5ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_model():\n",
    "    '''\n",
    "    Returns a random sequence model where all logits are equal.\n",
    "    '''\n",
    "    input_chains = tf.keras.layers.Input(shape=(None,), name='input_ids', dtype='int32')\n",
    "    input_ = tf.keras.layers.Input(shape=(None,), name='attention_mask', dtype='int32')\n",
    "    batch, seq_len = tf.shape(input_chains)\n",
    "    logits = tf.ones((batch, seq_len, 20), dtype=tf.float32)\n",
    "\n",
    "    random_model = tf.keras.models.Model(inputs={'input_ids': input_chains,'attention_mask': input_},\n",
    "                                         outputs={'logits': logits},\n",
    "                                         name='Random Model')\n",
    "\n",
    "    return random_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3557fc9-7360-4b8c-a325-6fe4257518e6",
   "metadata": {},
   "source": [
    "A random sequence model is constructed below.\n",
    "\n",
    "Recall, transformers tokenisers return two outputs (default) consisting of the tokenised sequence and an attention mask. To make the model compatible with DirectedEvolution we make the model accept these as well. *Note the attention mask does not change the returned probabilities.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "30d43127-03be-4269-9e69-d3073b0e7638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Random Model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)      [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " tf.compat.v1.shape_1 (TFOp  (2,)                         0         ['input_ids[0][0]']           \n",
      " Lambda)                                                                                          \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_2  ()                           0         ['tf.compat.v1.shape_1[0][0]']\n",
      "  (SlicingOpLambda)                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_3  ()                           0         ['tf.compat.v1.shape_1[0][0]']\n",
      "  (SlicingOpLambda)                                                                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer  [(None, None)]               0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.ones_1 (TFOpLambda)      (None, None, 20)             0         ['tf.__operators__.getitem_2[0\n",
      "                                                                    ][0]',                        \n",
      "                                                                     'tf.__operators__.getitem_3[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 0 (0.00 Byte)\n",
      "Trainable params: 0 (0.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "random_p_model = random_model()\n",
    "print(random_p_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6d7f72-1465-47b2-a205-61bff236ba2d",
   "metadata": {},
   "source": [
    "<a id=\"section4\"></a>\n",
    "\n",
    "## Generate 100 variants of the short peptide using the random and blosum models\n",
    "\n",
    "Below is code to generate 100 variants of the short peptide based on blosum model derived probabilities. Since the vocabulary for either case does not use a mask token we do not use masked marginals *as there is no masking*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "802f9a31-5797-4793-9b48-2871c8d7e596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 148ms/Directed Evolution Steps\n",
      "2/2 [==============================] - 0s 160ms/Directed Evolution Steps\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a weighted sampler.\n",
    "variant_sampler = WeightedDirectedEvolutionSampler(temperature=1)\n",
    "n_variants = 100\n",
    "# Use the Directed Evolution sampler.\n",
    "# Lets make 100 variants.\n",
    "short_peptides = {\n",
    "    'input_ids': tf.repeat(short_peptide, axis=0, repeats=n_variants),\n",
    "    'attention_mask': tf.repeat(attention_mask, axis=0, repeats=n_variants)\n",
    "}\n",
    "\n",
    "variants_blosum, mutational_probabilities, logits = variant_sampler(sequence=short_peptides,\n",
    "                                                                    model=blosum_p_model,\n",
    "                                                                    max_steps=2, \n",
    "                                                                    mask_token=21, \n",
    "                                                                    batch=100, \n",
    "                                                                    masked_marginals=False)\n",
    "\n",
    "variants_random, mutational_probabilities, logits = variant_sampler(sequence=short_peptides,\n",
    "                                                                    model=random_p_model,\n",
    "                                                                    max_steps=2,\n",
    "                                                                    mask_token=21,\n",
    "                                                                    batch=100, \n",
    "                                                                    masked_marginals=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dc76b5-7c2d-4a37-b49f-301d885cc0ab",
   "metadata": {},
   "source": [
    "We can inspect the variant sequences generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "eec47065-4379-483f-9f09-7aed1cf23180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['E G C', 'F A C', 'A M L', 'H V C', 'G C C', 'T A G', 'I M C', 'A A C', 'Q M C', 'E A G']\n"
     ]
    }
   ],
   "source": [
    "print(tokeniser.batch_decode(variants_random[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9193d2c2-8793-4142-ba42-f1e87ffad760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T A A', 'R V C', 'S A C', 'A A R', 'A G G', 'H A F', 'S W C', 'W A A', 'A T R', 'N F C']\n"
     ]
    }
   ],
   "source": [
    "print(tokeniser.batch_decode(variants_blosum[:10]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
